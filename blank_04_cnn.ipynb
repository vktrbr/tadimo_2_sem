{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u49LZV9dKFWi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "403Mis3LAgED"
   },
   "source": [
    "## 1. Классификация предметов одежды (датасет Fashion MNIST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mn6r8tMDQsaU"
   },
   "source": [
    "\n",
    "### 1.1 Решить задачу классификации, не используя сверточные слои. \n",
    "* Предложить архитектуру модели для решения задачи\n",
    "* Посчитать количество параметров модели.\n",
    "* Обучить модель\n",
    "* Вывести график функции потерь по эпохам. \n",
    "* Используя тестовое множество\n",
    "\n",
    "  * Продемонстрировать работу модели: вывести несколько изображений, указать над ними правильный класс и класс, предсказанный моделью. \n",
    "\n",
    "  * Вывести матрицу ошибок.\n",
    "\n",
    "  * Вывести значение accuracy на тестовом множестве.\n",
    "* Сохранить модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset FashionMNIST\n    Number of datapoints: 60000\n    Root location: ./fashion_mnist/train\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n           )"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train = datasets.FashionMNIST('./fashion_mnist/train', train=True,\n",
    "                                    transform=transforms.Compose([transforms.ToTensor()]))\n",
    "mnist_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset FashionMNIST\n    Number of datapoints: 10000\n    Root location: ./fashion_mnist/test\n    Split: Test\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n           )"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test = datasets.FashionMNIST('./fashion_mnist/test', train=False,\n",
    "                                   transform=transforms.Compose([transforms.ToTensor()]))\n",
    "mnist_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "device = 'cpu' #  'mps' if torch.has_mps else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(28 * 28, 2048),\n",
    "    torch.nn.LeakyReLU(0.05),\n",
    "    torch.nn.Linear(2048, 32),\n",
    "    torch.nn.LeakyReLU(2),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(32, len(mnist_train.classes))\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def test_accuracy(_model, _test_loader, _loss_func) -> [float, float]:\n",
    "    \"\"\" Возвращает два числа -- точность и лосс \"\"\"\n",
    "    acc = 0\n",
    "    _loss = 0\n",
    "    with torch.no_grad():\n",
    "        for j, (x, y) in enumerate(_test_loader, 1):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(x)\n",
    "            acc += torch.sum(torch.eq(pred.argmax(dim=1).long().to('cpu'), y.to('cpu')))\n",
    "            _loss += _loss_func(pred, y)\n",
    "\n",
    "    return acc / _test_loader.dataset.data.shape[0], _loss / j"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def train(_model, train_data, test_data, epochs: int = 30, batch_size: int = 20_000) -> [torch.nn.Module, pd.DataFrame]:\n",
    "    optimizer = torch.optim.Adam(_model.parameters(), weight_decay=0.00001)\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "    log = pd.DataFrame(columns=['epoch', 'train_loss', 'test_accuracy', 'test_loss'])\n",
    "\n",
    "    for i in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for j, (x, y) in enumerate(loader, 1):\n",
    "            x = x.data.to(device)\n",
    "            y = y.data.to(device)\n",
    "            y_pred = _model(x.data)\n",
    "            running_loss = loss(y_pred, y)\n",
    "            running_loss.backward()\n",
    "\n",
    "            epoch_loss += running_loss.item()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        test_acc, test_loss = test_accuracy(model, test_loader, loss)\n",
    "        log.loc[i] = [i + 1, epoch_loss / j, test_acc.item(), test_loss.item()]\n",
    "\n",
    "        print(f'EPOCH: {i + 1 :3d}  |  LOSS: {epoch_loss / j: .4f}', end='  |  ')\n",
    "        print(f'TEST LOSS: {test_loss:0.4f}  |  TEST ACCURACY: {test_acc:0.4f}')\n",
    "\n",
    "    return _model, log"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [68]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model, log \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmnist_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmnist_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [67]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(_model, train_data, test_data, epochs, batch_size)\u001B[0m\n\u001B[1;32m     15\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m _model(x\u001B[38;5;241m.\u001B[39mdata)\n\u001B[1;32m     16\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m loss(y_pred, y)\n\u001B[0;32m---> 17\u001B[0m \u001B[43mrunning_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m running_loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     20\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.9/site-packages/torch/_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model, log = train(model, mnist_train, mnist_test, 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.line(log, x='epoch', y=['train_loss', 'test_loss'], title='<b>LOSS PLOT</b>').show(renderer='png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "px.line(log, x='epoch', y='test_accuracy', title='<b>TEST ACCURACY</b>').show(renderer='png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.to('cpu')\n",
    "right_ans = torch.zeros(mnist_test.data.shape[0]).to('cpu')\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for x, _ in DataLoader(mnist_test, batch_size=2000):\n",
    "        x = x.to('cpu')\n",
    "        right_ans[i:i + x.shape[0]] += model(x).argmax(dim=1)\n",
    "        i += x.shape[0]\n",
    "\n",
    "cm = confusion_matrix(mnist_test.targets.numpy(), right_ans.long().numpy())\n",
    "px.imshow(cm).show(renderer='png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/mnist_linear')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(21)\n",
    "for i in np.random.randint(0, mnist_test.targets.shape[0], 3):\n",
    "    id_class = mnist_test.targets[i]\n",
    "    class_name = mnist_test.classes[id_class]\n",
    "\n",
    "    id_predicted = model(mnist_test.data[i:i + 1].float()).argmax(dim=1).item()\n",
    "    class_name_predicted = mnist_test.classes[id_predicted]\n",
    "    print(f'CLASS : {id_class} ({class_name})  |  '\n",
    "          f'PREDICTED CLASS : {id_predicted} ({class_name_predicted})')\n",
    "    px.imshow(mnist_test.data[i], color_continuous_scale='gray').show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmsMQUPP-XUh"
   },
   "source": [
    "### 1.2 Решить задачу 1.1, используя сверточную нейронную сеть. \n",
    "* Добиться значения accuracy на тестовом множестве не менее 90%\n",
    "* Визуализировать результаты работы первого сверточного слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset FashionMNIST\n    Number of datapoints: 60000\n    Root location: ./fashion_mnist/train\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n           )"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train = datasets.FashionMNIST('./fashion_mnist/train', train=True,\n",
    "                                    transform=transforms.Compose([transforms.ToTensor()]))\n",
    "mnist_test = datasets.FashionMNIST('./fashion_mnist/test', train=False,\n",
    "                                    transform=transforms.Compose([transforms.ToTensor()]))\n",
    "mnist_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "p = 0.2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "\n",
    "    torch.nn.BatchNorm2d(1),\n",
    "\n",
    "    torch.nn.Conv2d(1, 64, (3, 3), padding=1),\n",
    "    torch.nn.BatchNorm2d(64),\n",
    "    torch.nn.ReLU(),\n",
    "\n",
    "    torch.nn.Conv2d(64, 64, (3, 3), padding=1),\n",
    "    torch.nn.MaxPool2d((2, 2)),\n",
    "    torch.nn.BatchNorm2d(64),\n",
    "    torch.nn.ReLU(),\n",
    "\n",
    "    # torch.nn.Dropout(p),\n",
    "    #\n",
    "    # torch.nn.Conv2d(32, 64, (3, 3), padding=1),\n",
    "    # torch.nn.BatchNorm2d(64),\n",
    "    # torch.nn.LeakyReLU(0.4),\n",
    "\n",
    "    #\n",
    "    # torch.nn.Conv2d(64, 64, (3, 3), padding=1),\n",
    "    # torch.nn.MaxPool2d((2, 2)),\n",
    "    # torch.nn.BatchNorm2d(64),\n",
    "    # torch.nn.LeakyReLU(0.4),\n",
    "\n",
    "    # torch.nn.Conv2d(32, 64, (3, 3), padding=1),\n",
    "    # torch.nn.MaxPool2d((2, 2)),\n",
    "    # torch.nn.LeakyReLU(0.4),\n",
    "\n",
    "    torch.nn.Dropout(p),\n",
    "\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(32 * 32, 2048),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(2048, 10)\n",
    "\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2157516)"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_params = 0\n",
    "for param in model.parameters():\n",
    "    number_params += torch.prod(torch.tensor(param.shape))\n",
    "\n",
    "number_params  # Количество параметров в сети"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [87]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model, log \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmnist_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmnist_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m30_000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [69]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(_model, train_data, test_data, epochs, batch_size)\u001B[0m\n\u001B[1;32m     13\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     14\u001B[0m y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 15\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[43m_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m loss(y_pred, y)\n\u001B[1;32m     17\u001B[0m running_loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 459\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/conv.py:455\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    451\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    452\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    453\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    454\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 455\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    456\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model, log = train(model, mnist_train, mnist_test, 10, 30_000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm6_9B3ZKLq9"
   },
   "source": [
    "##  2. Классификация изображений (датасет CIFAR 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNTZa9yWQvSF"
   },
   "source": [
    "\n",
    "### 2.1 Решить задачу классификации, не используя сверточные слои. \n",
    "\n",
    "* Нормализовать данные (если необходимо)\n",
    "* Предложить архитектуру модели для решения задачи\n",
    "* Посчитать количество параметров модели.\n",
    "* Обучить модель\n",
    "* Вывести график функции потерь по эпохам. \n",
    "* Используя тестовое множество\n",
    "\n",
    "  * Продемонстрировать работу модели: вывести несколько изображений, указать над ними правильный класс и класс, предсказанный моделью. \n",
    "\n",
    "  * Вывести матрицу ошибок.\n",
    "\n",
    "  * Вывести значение accuracy на тестовом множестве.\n",
    "* Сохранить модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIxHLbKcDCFA"
   },
   "source": [
    "### 2.2 Решить задачу 2.1, используя сверточную нейронную сеть. \n",
    "* Добиться значения accuracy на тестовом множестве не менее 70%.\n",
    "* Визуализировать результаты работы первого сверточного слоя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPg8k4YTFOgc"
   },
   "source": [
    "## 3. Загрузка изображений из внешних источников"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hLnpe_570wd"
   },
   "source": [
    "### 3.1 Решить задачу классификации обезьян (датасет [monkey.zip](https://disk.yandex.ru/d/OxYgY4S7aR6ulQ)).\n",
    "* Загрузить архив с данными на диск\n",
    "* Создать датасет на основе файлов при помощи `torchvision.datasets.ImageFolder`\n",
    "* Преобразовать изображения к тензорами одного размера (например, 400х400). Потестировать другие преобразования из `torchvision.transforms`\n",
    "* Предложить архитектуру модели для решения задачи. Обучить модель.\n",
    "* Используя тестовое множество\n",
    "\n",
    "  * Продемонстрировать работу модели: вывести несколько изображений, указать над ними правильный класс и класс, предсказанный моделью. \n",
    "\n",
    "  * Вывести матрицу ошибок.\n",
    "\n",
    "  * Вывести значение accuracy на тестовом множестве.\n",
    "  * Добиться значения accuracy на тестовом множестве не менее 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1616695933807,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "iXnzblkFFN5K",
    "outputId": "cbbabfdc-a8c4-41c6-86a8-f7e99748a612"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17268,
     "status": "ok",
     "timestamp": 1616695954172,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     },
     "user_tz": -180
    },
    "id": "4Er_FLV6Hrqq",
    "outputId": "5ece4316-16e6-458c-eb94-c0604e19ea24"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "zf = zipfile.ZipFile('drive/MyDrive/datasets/monkeys.zip')\n",
    "for file in tqdm(zf.infolist()):\n",
    "    zf.extract(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-tMy4qJ9j1k"
   },
   "source": [
    "### 3.2 Решить задачу классификации собак и кошек (датасет [cats_dogs.zip](https://disk.yandex.ru/d/wQtt5O1JF9ctnA)).\n",
    "* Загрузить архив с данными на диск\n",
    "* Создать датасет на основе файлов при помощи `torchvision.datasets.ImageFolder`\n",
    "* Преобразовать изображения к тензорами одного размера (например, 400х400). Потестировать другие преобразования из `torchvision.transforms`\n",
    "* Предложить архитектуру модели для решения задачи. Обучить модель.\n",
    "* Используя тестовое множество\n",
    "\n",
    "  * Продемонстрировать работу модели: вывести несколько изображений, указать над ними правильный класс и класс, предсказанный моделью. \n",
    "\n",
    "  * Вывести матрицу ошибок.\n",
    "\n",
    "  * Вывести значение accuracy на тестовом множестве.\n",
    "  * Добиться значения accuracy на тестовом множестве не менее 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqvleAgxfC6B"
   },
   "source": [
    "# 4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjNUwuho9tm9"
   },
   "source": [
    "### 4.1 Решить задачу 3.1, воспользовавшись предобученной моделью VGG16\n",
    "* Загрузить данные для обучения\n",
    "* Преобразования: размер 224x224, нормализация с параметрами `mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)`\n",
    "* Заменить последний полносвязный слой модели в соответствии с задачей\n",
    "* Дообучить модель (не замораживать веса). Вычислить значение accuracy на тестовом множестве\n",
    "* Дообучить модель (заморозить все веса, кроме последнего блока слоев (`classifier`)). \n",
    "* Вычислить значение accuracy на тестовом множестве.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt_hXy5a5CE8"
   },
   "source": [
    "### 4.2 Решить задачу 3.2, воспользовавшись подходящей предобученной моделью\n",
    "* Не использовать VGG16 (вместо нее можно взять resnet18 или другую)\n",
    "* Загрузить данные для обучения\n",
    "* Преобразования: размер 224x224, нормализация с параметрами `mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)`\n",
    "* Заменить последний полносвязный слой модели в соответствии с задачей\n",
    "* Дообучить модель. \n",
    "* Вычислить значение accuracy на тестовом множестве (добиться значения не меньше 97-98%)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMF+zO/4VvRSXLWjIV8BHI2",
   "collapsed_sections": [],
   "name": "blank_04_cnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
