{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "blank__05_NLP_1_intro.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyMJ70DQQj2X/XaG2BMq6jy8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "XtFQP3RNll3c"
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "443CZCLp0_sE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1618423490594,
     "user_tz": -180,
     "elapsed": 7026,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     }
    },
    "outputId": "ddda53b2-a593-4d13-ee48-cfed23e613b1"
   },
   "source": [
    "nltk.download('punkt')"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/victorbarbarich/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqDHq_AEjRZ1"
   },
   "source": [
    "## 1. Представление и предобработка текстовых данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaki7efDpmXo"
   },
   "source": [
    "1.1 Операции по предобработке:\n",
    "* токенизация\n",
    "* стемминг / лемматизация\n",
    "* удаление стоп-слов\n",
    "* удаление пунктуации\n",
    "* приведение к нижнему регистру\n",
    "* любые другие операции над текстом"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nHRy4jpYphEr"
   },
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lMMzGhq0ikz1"
   },
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUhfertRtXE5"
   },
   "source": [
    "Реализовать функцию `preprocess_text(text: str)`, которая:\n",
    "* приводит строку к нижнему регистру\n",
    "* заменяет все символы, кроме a-z, A-Z и знаков .,!? на пробел\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "'select your preferences and run the install command. stable represents the most currently tested and supported version of pytorch. note that libtorch is only available for c  '"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Приводит строку к нижнему регистру и заменяет все символы, кроме a-z, A-Z и знаков .,!? на пробел\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'[^a-zA-Z,.?!]')\n",
    "    _text = _text.lower()\n",
    "    _text = re.sub(pattern, ' ', _text)\n",
    "    return _text\n",
    "\n",
    "\n",
    "preprocess_text(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2Dt1ssIqckC"
   },
   "source": [
    "1.2 Представление текстовых данных при помощи бинарного кодирования\n",
    "\n",
    "\n",
    "Представить первое предложение из `text` в виде тензора `sentence_t`: `sentence_t[i] == 1`, если __слово__ с индексом `i` присуствует в предложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'currently', 'and', 'libtorch', 'stable', 'the', 'select', 'your', 'most', 'available', 'version', 'c', 'only', 'of', 'install', 'represents', 'tested', 'preferences', 'pytorch.', 'note', 'for', 'is', 'that', 'command.', 'supported']\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def get_binary_coded_arr(words: list[str], _word_bag: list[str]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Возвращает массив из 0 или 1\n",
    "\n",
    "    :param words: слова из предложения\n",
    "    :param _word_bag: общий набор всех слов\n",
    "    :return: список из 0 и 1\n",
    "    \"\"\"\n",
    "\n",
    "    return [(word in _word_bag) * 1 for word in words]\n",
    "\n",
    "\n",
    "word_bag = list(set(preprocess_text(text).split()))\n",
    "sentence_1 = get_binary_coded_arr(text.split('.')[0], word_bag)\n",
    "\n",
    "print(word_bag)\n",
    "print(sentence_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2Nz_zcgw3N4"
   },
   "source": [
    "## 2. Классификация фамилий по национальности\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
    "\n",
    "2.1 Считать файл `surnames/surnames.csv`. \n",
    "\n",
    "2.2 Закодировать национальности числами, начиная с 0.\n",
    "\n",
    "2.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
    "\n",
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "\n",
    "2.6. Обучить классификатор.\n",
    "\n",
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "    surname nationality\n0  Woodford     English\n1      Coté      French",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "surnames = pd.read_csv('data/surnames.csv')\n",
    "surnames.head(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# 2\n",
    "nationalities = surnames.nationality.unique()\n",
    "nationality_to_idx = dict(zip(nationalities, range(len(nationalities))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "# 3\n",
    "surnames_train, surnames_test = train_test_split(surnames, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kUkSZkDqxNYS"
   },
   "source": [
    "class Vocab:\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        data = data[['surname']].drop_duplicates().reset_index(drop=True)\n",
    "        data.surname = data.surname.str.lower()\n",
    "        self.idx_to_token = data.surname.to_dict()\n",
    "        self.token_to_idx = data.reset_index().set_index('surname')['index'].to_dict()\n",
    "        self.vocab_len = data.shape[0]\n",
    "\n",
    "\n",
    "vocab_test = Vocab(surnames.head(3))\n",
    "vocab_test.vocab_len, vocab_test.token_to_idx, vocab_test.idx_to_token"
   ],
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "(3,\n {'woodford': 0, 'coté': 1, 'kore': 2},\n {0: 'woodford', 1: 'coté', 2: 'kore'})"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WCaRK1QHxe0A",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1619117849212,
     "user_tz": -180,
     "elapsed": 1303,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     }
    },
    "outputId": "5d1243af-d0dd-4922-9468-9618f5df4605"
   },
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "    def __init__(self, x, y, vocab: Vocab):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, surname: str):\n",
    "        \"\"\" Генерирует представление фамилии surname в при помощи бинарного кодирования (см. 1.2) \"\"\"\n",
    "        output_vector = [0 for _ in range(self.vocab.vocab_len)]\n",
    "        try:\n",
    "            output_vector[self.vocab.token_to_idx[surname.lower()]] = 1\n",
    "        except IndexError:\n",
    "            pass\n",
    "        return output_vector\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.vectorize(self.x[idx])).float(), self.y[idx]\n"
   ],
   "execution_count": 112,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0]\n",
      "10980 9041\n",
      "[0, 1, 0, 0] French\n"
     ]
    }
   ],
   "source": [
    "dataset_testing = SurnamesDataset(surnames.surname, surnames.nationality, Vocab(surnames))\n",
    "\n",
    "print(dataset_testing.vectorize('Woodford')[:4])\n",
    "print(len(dataset_testing), dataset_testing.vocab.vocab_len)\n",
    "print(dataset_testing[1][0][:4], dataset_testing[1][1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sigmoid(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [115]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m vocab_test_loader \u001B[38;5;241m=\u001B[39m SurnamesDataset(surnames_test\u001B[38;5;241m.\u001B[39msurname, surnames_test\u001B[38;5;241m.\u001B[39mnationality, Vocab(surnames))\n\u001B[1;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[1;32m      5\u001B[0m     torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mSigmoid(),\n\u001B[1;32m      6\u001B[0m     torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mLinear(vocab_train_loader\u001B[38;5;241m.\u001B[39mvocab\u001B[38;5;241m.\u001B[39mvocab_len, \u001B[38;5;28mlen\u001B[39m(nationality_to_idx)),\n\u001B[1;32m      7\u001B[0m     torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mSoftmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      8\u001B[0m )\n\u001B[0;32m---> 10\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvocab_train_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pytorch-test/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/pytorch-test/env/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 141\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/pytorch-test/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/pytorch-test/env/lib/python3.8/site-packages/torch/nn/modules/activation.py:293\u001B[0m, in \u001B[0;36mSigmoid.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msigmoid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: sigmoid(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "vocab_train_loader = SurnamesDataset(surnames_train.surname, surnames_train.nationality, Vocab(surnames))\n",
    "vocab_test_loader = SurnamesDataset(surnames_test.surname, surnames_test.nationality, Vocab(surnames))\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(vocab_train_loader.vocab.vocab_len, len(nationality_to_idx)),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "model(next(iter(vocab_train_loader))[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLmDB3fJtVox"
   },
   "source": [
    "## 3. Классификация обзоров ресторанов\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/nY1o70JtAuYa8g\n",
    "\n",
    "3.1 Считать файл `yelp/raw_train.csv`. Оставить от исходного датасета 10% строчек.\n",
    "\n",
    "3.2 Воспользоваться функцией `preprocess_text` из 1.1 для обработки текста отзыва. Закодировать рейтинг числами, начиная с 0.\n",
    "\n",
    "3.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "3.4 Реализовать класс `Vocab` (токен = слово)\n",
    "\n",
    "3.5 Реализовать класс `ReviewDataset`\n",
    "\n",
    "3.6 Обучить классификатор\n",
    "\n",
    "3.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_lCTSKZgu68K"
   },
   "source": [
    "class Vocab:\n",
    "    def __init__(self, data):\n",
    "        self.idx_to_token = ...\n",
    "        self.token_to_idx = ...\n",
    "        self.vocab_len = ..."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WXLmCDvcvRmb"
   },
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, X, y, vocab: Vocab):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        '''Генерирует представление отзыва review при помощи бинарного кодирования (см. 1.2)'''\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return ..."
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
