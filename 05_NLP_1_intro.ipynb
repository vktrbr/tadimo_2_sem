{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "blank__05_NLP_1_intro.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyMJ70DQQj2X/XaG2BMq6jy8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "XtFQP3RNll3c"
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "443CZCLp0_sE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1618423490594,
     "user_tz": -180,
     "elapsed": 7026,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     }
    },
    "outputId": "ddda53b2-a593-4d13-ee48-cfed23e613b1"
   },
   "source": [
    "nltk.download('punkt')"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqDHq_AEjRZ1"
   },
   "source": [
    "## 1. Представление и предобработка текстовых данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaki7efDpmXo"
   },
   "source": [
    "1.1 Операции по предобработке:\n",
    "* токенизация\n",
    "* стемминг / лемматизация\n",
    "* удаление стоп-слов\n",
    "* удаление пунктуации\n",
    "* приведение к нижнему регистру\n",
    "* любые другие операции над текстом"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nHRy4jpYphEr"
   },
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lMMzGhq0ikz1"
   },
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUhfertRtXE5"
   },
   "source": [
    "Реализовать функцию `preprocess_text(text: str)`, которая:\n",
    "* приводит строку к нижнему регистру\n",
    "* заменяет все символы, кроме a-z, A-Z и знаков .,!? на пробел\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'select your preferences and run the install command. stable represents the most currently tested and supported version of pytorch. note that libtorch is only available for c  '"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Приводит строку к нижнему регистру и заменяет все символы, кроме a-z, A-Z и знаков .,!? на пробел\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'[^a-zA-Z,.?!]')\n",
    "    _text = _text.lower()\n",
    "    _text = re.sub(pattern, ' ', _text)\n",
    "    return _text\n",
    "\n",
    "\n",
    "preprocess_text(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2Dt1ssIqckC"
   },
   "source": [
    "1.2 Представление текстовых данных при помощи бинарного кодирования\n",
    "\n",
    "\n",
    "Представить первое предложение из `text` в виде тензора `sentence_t`: `sentence_t[i] == 1`, если __слово__ с индексом `i` присуствует в предложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['most', 'command.', 'run', 'the', 'select', 'available', 'tested', 'preferences', 'for', 'version', 'is', 'your', 'of', 'that', 'libtorch', 'c', 'and', 'currently', 'only', 'stable', 'note', 'install', 'represents', 'supported', 'pytorch.']\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def get_binary_coded_arr(words: list[str], _word_bag: list[str]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Возвращает массив из 0 или 1\n",
    "\n",
    "    :param words: слова из предложения\n",
    "    :param _word_bag: общий набор всех слов\n",
    "    :return: список из 0 и 1\n",
    "    \"\"\"\n",
    "\n",
    "    return [(word in _word_bag) * 1 for word in words]\n",
    "\n",
    "\n",
    "word_bag = list(set(preprocess_text(text).split()))\n",
    "sentence_1 = get_binary_coded_arr(text.split('.')[0], word_bag)\n",
    "\n",
    "print(word_bag)\n",
    "print(sentence_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2Nz_zcgw3N4"
   },
   "source": [
    "## 2. Классификация фамилий по национальности\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
    "\n",
    "2.1 Считать файл `surnames/surnames.csv`. \n",
    "\n",
    "2.2 Закодировать национальности числами, начиная с 0.\n",
    "\n",
    "2.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
    "\n",
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "\n",
    "2.6. Обучить классификатор.\n",
    "\n",
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "    surname nationality\n0  Woodford     English\n1      Coté      French",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>surname</th>\n      <th>nationality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woodford</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Coté</td>\n      <td>French</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "surnames = pd.read_csv('data/surnames.csv')\n",
    "surnames.head(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# 2\n",
    "nationalities = surnames.nationality.unique()\n",
    "nationality_to_idx = dict(zip(nationalities, range(len(nationalities))))\n",
    "idx_to_nationality = dict(zip(range(len(nationalities)), nationalities))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 3\n",
    "surnames_train, surnames_test = train_test_split(surnames, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kUkSZkDqxNYS"
   },
   "source": [
    "# 4\n",
    "class Vocab:\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        data = data[['surname']].drop_duplicates().reset_index(drop=True)\n",
    "        data.surname = data.surname.str.lower()\n",
    "        self.idx_to_token = data.surname.to_dict()\n",
    "        self.token_to_idx = data.reset_index().set_index('surname')['index'].to_dict()\n",
    "        self.vocab_len = data.shape[0]\n",
    "\n",
    "\n",
    "vocab_test = Vocab(surnames.head(3))\n",
    "vocab_test.vocab_len, vocab_test.token_to_idx, vocab_test.idx_to_token"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(3,\n {'woodford': 0, 'coté': 1, 'kore': 2},\n {0: 'woodford', 1: 'coté', 2: 'kore'})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WCaRK1QHxe0A",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1619117849212,
     "user_tz": -180,
     "elapsed": 1303,
     "user": {
      "displayName": "Никита Блохин",
      "photoUrl": "",
      "userId": "16402972581398673009"
     }
    },
    "outputId": "5d1243af-d0dd-4922-9468-9618f5df4605"
   },
   "source": [
    "# 5\n",
    "class SurnamesDataset(Dataset):\n",
    "    def __init__(self, x, y, vocab: Vocab):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, surname: str):\n",
    "        \"\"\" Генерирует представление фамилии surname в при помощи бинарного кодирования (см. 1.2) \"\"\"\n",
    "        output_vector = [0 for _ in range(self.vocab.vocab_len)]\n",
    "        try:\n",
    "            output_vector[self.vocab.token_to_idx[surname.lower()]] = 1\n",
    "        except IndexError:\n",
    "            pass\n",
    "        return output_vector\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.vectorize(self.vocab.idx_to_token[idx])\n",
    "        x = torch.FloatTensor(x)\n",
    "        return x, torch.tensor(nationality_to_idx[self.y.iloc[idx]])\n"
   ],
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0]\n",
      "10980 9041\n",
      "tensor([0., 1., 0., 0.]) tensor(1)\n"
     ]
    }
   ],
   "source": [
    "dataset_testing = SurnamesDataset(surnames.surname, surnames.nationality, Vocab(surnames))\n",
    "\n",
    "print(dataset_testing.vectorize('Woodford')[:4])\n",
    "\n",
    "print(len(dataset_testing), dataset_testing.vocab.vocab_len)\n",
    "print(dataset_testing[1][0][:4], dataset_testing[1][1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0347, 0.0548, 0.0834, 0.0599, 0.0586, 0.0744, 0.0788, 0.0547, 0.0521,\n         0.0528, 0.0438, 0.0496, 0.0576, 0.0471, 0.0407, 0.0435, 0.0462, 0.0675]],\n       grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_train_dataset= SurnamesDataset(surnames_train.surname, surnames_train.nationality, Vocab(surnames))\n",
    "vocab_test_dataset = SurnamesDataset(surnames_test.surname, surnames_test.nationality, Vocab(surnames))\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(vocab_train_dataset.vocab.vocab_len, len(nationality_to_idx)),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "model(next(iter(DataLoader(vocab_train_dataset)))[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:   1 \t LOSS: 2.7111\n",
      "EPOCH:   3 \t LOSS: 2.7112\n",
      "EPOCH:   5 \t LOSS: 2.7113\n",
      "EPOCH:   7 \t LOSS: 2.7113\n",
      "EPOCH:   9 \t LOSS: 2.7111\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "loss_log = []\n",
    "\n",
    "for i in range(10):\n",
    "    epoch_loss = 0\n",
    "    j = 1\n",
    "\n",
    "    for j, (batch_x, batch_y) in enumerate(DataLoader(vocab_train_dataset, batch_size=32, shuffle=True), 1):\n",
    "\n",
    "        y_pred = model(batch_x)\n",
    "        running_loss = loss(y_pred, batch_y)\n",
    "        epoch_loss += running_loss.item()\n",
    "\n",
    "        running_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss /= j\n",
    "    if i % 2 == 0:\n",
    "\n",
    "        print(f'EPOCH: {i + 1:3d} \\t LOSS: {epoch_loss:0.4f}')\n",
    "\n",
    "    loss_log.append(epoch_loss)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# задание абсолютно некорректное\n",
    "# сохраним в гите это решение и идем дальше"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLmDB3fJtVox"
   },
   "source": [
    "## 3. Классификация обзоров ресторанов\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/nY1o70JtAuYa8g\n",
    "\n",
    "3.1 Считать файл `yelp/raw_train.csv`. Оставить от исходного датасета 10% строчек.\n",
    "\n",
    "3.2 Воспользоваться функцией `preprocess_text` из 1.1 для обработки текста отзыва. Закодировать рейтинг числами, начиная с 0.\n",
    "\n",
    "3.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "3.4 Реализовать класс `Vocab` (токен = слово)\n",
    "\n",
    "3.5 Реализовать класс `ReviewDataset`\n",
    "\n",
    "3.6 Обучить классификатор\n",
    "\n",
    "3.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_lCTSKZgu68K"
   },
   "source": [
    "class Vocab:\n",
    "    def __init__(self, data):\n",
    "        self.idx_to_token = ...\n",
    "        self.token_to_idx = ...\n",
    "        self.vocab_len = ..."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WXLmCDvcvRmb"
   },
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, X, y, vocab: Vocab):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        '''Генерирует представление отзыва review при помощи бинарного кодирования (см. 1.2)'''\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return ..."
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
